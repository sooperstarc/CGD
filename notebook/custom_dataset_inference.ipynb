{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yoo/works/CGD\n"
     ]
    }
   ],
   "source": [
    "# 작업 폴더 설정\n",
    "%cd /home/yoo/works/CGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/custom 폴더 하위에 있는 이미지 파일(jpg, JPG)를 하나의 텍스트 파일에 정리\n",
    "\n",
    "# import os\n",
    "\n",
    "# search_path = 'data/custom'\n",
    "\n",
    "# txt = open(\"{}/test.txt\".format(search_path), 'w')\n",
    "# txt.write(\"image_id\\tclass_id\\tsuper_class_id\\tpath\")\n",
    "\n",
    "# idx = 0\n",
    "\n",
    "# for (path, dir, files) in os.walk(search_path):\n",
    "\n",
    "#     for filename in files:\n",
    "#         ext = os.path.splitext(filename)[-1]\n",
    "\n",
    "#         if path.split('/')[-1] != 'uncropped':\n",
    "            \n",
    "#             if ext in ['.jpg', '.JPG']:\n",
    "#                 idx += 1\n",
    "#                 txt.write(\"\\n\")\n",
    "#                 txt.write(f\"_\\t{idx}\\t_\\t{os.path.join(path, filename)}\")\n",
    "#                 # txt.write(f\"_\\t{idx}\\t_\\t{filename}\")\n",
    "\n",
    "# txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 폴더를 제외하고 하위 폴더에 있는 이미지 파일 목록을 텍스트 파일로 저장\n",
    "# 코드가 반복문을 이중으로 쓰고 있어서 효율화할 필요가 있음(!!!)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "search_path = 'data/custom'\n",
    "except_dirs = ['backup', 'uncropped']\n",
    "\n",
    "with open(f\"{search_path}/test.txt\", 'w') as txt:\n",
    "    txt.write(\"image_id\\tclass_id\\tsuper_class_id\\tpath\")\n",
    "    \n",
    "    classes = []\n",
    "    files = []\n",
    "\n",
    "    dirs = [dir for dir in os.listdir(search_path) if dir not in except_dirs and os.path.isdir(os.path.join(search_path, dir))]\n",
    "\n",
    "    for dir in dirs:\n",
    "        read_path = os.path.join(search_path, dir)\n",
    "        for dp, dn, filenames in os.walk(read_path):\n",
    "            for f in filenames:\n",
    "                if f.endswith(('.jpg', '.JPG')):\n",
    "                    files.append(os.path.join(dp, f))\n",
    "\n",
    "    files.sort()\n",
    "\n",
    "    class_ids = sorted(set(os.path.basename(f).split('_')[0] for f in files))\n",
    "\n",
    "    for image_id, path in enumerate(files, 1):\n",
    "\n",
    "        class_id = class_ids.index(os.path.basename(path).split('_')[0]) + 1\n",
    "\n",
    "        txt.write(\"\\n\")\n",
    "        txt.write(f\"{image_id}\\t{class_id}\\t_\\t{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_in_file(file_path, old_str, new_str):\n",
    "\n",
    "    # 파일 읽어들이기\n",
    "    fr = open(file_path, 'r')\n",
    "    lines = fr.readlines()\n",
    "    fr.close()\n",
    "    \n",
    "    # old_str -> new_str 치환\n",
    "    fw = open(file_path, 'w')\n",
    "    for line in lines:\n",
    "        fw.write(line.replace(old_str, new_str))\n",
    "    fw.close()\n",
    "\n",
    "# 호출: file1.txt 파일에서 comma(,) 없애기\n",
    "replace_in_file(\"data/custom/test.txt\", \"bicycle_final\", \"data/custom/bicycle\")\n",
    "replace_in_file(\"data/custom/train.txt\", \"bicycle_final\", \"data/custom/bicycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 생성 (uncropped_data_dicts.pth)\n",
    "\n",
    "import data_utils\n",
    "\n",
    "data_path = 'data/custom'\n",
    "\n",
    "data_utils.process_custom_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스 확인\n",
    "\n",
    "import torch\n",
    "\n",
    "pth_path = 'data/custom/uncropped_data_dicts.pth'\n",
    "\n",
    "data_base = torch.load(pth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base['train']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base['test']['11319']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "# Model Params: 27.83M FLOPs: 10.68G\n",
      "Train Epoch 1/20 - Loss:6.5348 - Acc:0.75%: 100%|█| 130/130 [00:41<00:00,  3.11i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.07it/s]\n",
      "Test Epoch 1/20 R@1:21.69% R@10:49.14% R@100:79.03% \n",
      "Train Epoch 2/20 - Loss:6.4913 - Acc:0.67%: 100%|█| 130/130 [00:42<00:00,  3.08i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.91it/s]\n",
      "Test Epoch 2/20 R@1:20.14% R@10:47.91% R@100:82.20% \n",
      "Train Epoch 3/20 - Loss:6.5058 - Acc:0.41%: 100%|█| 130/130 [00:41<00:00,  3.11i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.92it/s]\n",
      "Test Epoch 3/20 R@1:7.48% R@10:39.64% R@100:79.52% \n",
      "Train Epoch 4/20 - Loss:6.4254 - Acc:0.58%: 100%|█| 130/130 [00:41<00:00,  3.10i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.93it/s]\n",
      "Test Epoch 4/20 R@1:22.99% R@10:53.61% R@100:86.25% \n",
      "Train Epoch 5/20 - Loss:6.2578 - Acc:1.08%: 100%|█| 130/130 [00:42<00:00,  3.07i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.01it/s]\n",
      "Test Epoch 5/20 R@1:29.41% R@10:58.96% R@100:84.95% \n",
      "Train Epoch 6/20 - Loss:6.1197 - Acc:0.99%: 100%|█| 130/130 [00:39<00:00,  3.32i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:13<00:00,  9.70it/s]\n",
      "Test Epoch 6/20 R@1:29.84% R@10:56.65% R@100:83.72% \n",
      "Train Epoch 7/20 - Loss:5.9728 - Acc:2.04%: 100%|█| 130/130 [00:39<00:00,  3.32i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:13<00:00,  9.72it/s]\n",
      "Test Epoch 7/20 R@1:31.82% R@10:59.98% R@100:86.15% \n",
      "Train Epoch 8/20 - Loss:5.7626 - Acc:2.98%: 100%|█| 130/130 [00:38<00:00,  3.33i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:13<00:00,  9.73it/s]\n",
      "Test Epoch 8/20 R@1:38.00% R@10:64.44% R@100:88.71% \n",
      "Train Epoch 9/20 - Loss:5.5406 - Acc:4.33%: 100%|█| 130/130 [00:42<00:00,  3.09i\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.01it/s]\n",
      "Test Epoch 9/20 R@1:39.54% R@10:66.01% R@100:87.91% \n",
      "Train Epoch 10/20 - Loss:5.2938 - Acc:7.31%: 100%|█| 130/130 [00:42<00:00,  3.06\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.95it/s]\n",
      "Test Epoch 10/20 R@1:43.52% R@10:70.54% R@100:90.21% \n",
      "Train Epoch 11/20 - Loss:5.0647 - Acc:10.53%: 100%|█| 130/130 [00:42<00:00,  3.0\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.95it/s]\n",
      "Test Epoch 11/20 R@1:49.24% R@10:74.48% R@100:92.21% \n",
      "Train Epoch 12/20 - Loss:4.7657 - Acc:14.50%: 100%|█| 130/130 [00:42<00:00,  3.0\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.82it/s]\n",
      "Test Epoch 12/20 R@1:53.00% R@10:77.25% R@100:92.45% \n",
      "Train Epoch 13/20 - Loss:4.3762 - Acc:23.89%: 100%|█| 130/130 [00:41<00:00,  3.1\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.10it/s]\n",
      "Test Epoch 13/20 R@1:57.49% R@10:80.43% R@100:93.94% \n",
      "Train Epoch 14/20 - Loss:4.1880 - Acc:27.86%: 100%|█| 130/130 [00:42<00:00,  3.0\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.13it/s]\n",
      "Test Epoch 14/20 R@1:58.31% R@10:81.45% R@100:94.50% \n",
      "Train Epoch 15/20 - Loss:4.0841 - Acc:30.87%: 100%|█| 130/130 [00:41<00:00,  3.1\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.11it/s]\n",
      "Test Epoch 15/20 R@1:58.46% R@10:81.79% R@100:94.33% \n",
      "Train Epoch 16/20 - Loss:4.0041 - Acc:33.44%: 100%|█| 130/130 [00:41<00:00,  3.1\n",
      "processing test data: 100%|███████████████████| 130/130 [00:13<00:00,  9.29it/s]\n",
      "Test Epoch 16/20 R@1:59.35% R@10:82.77% R@100:94.55% \n",
      "Train Epoch 17/20 - Loss:3.8511 - Acc:38.77%: 100%|█| 130/130 [00:42<00:00,  3.0\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.82it/s]\n",
      "Test Epoch 17/20 R@1:59.28% R@10:82.61% R@100:94.67% \n",
      "Train Epoch 18/20 - Loss:3.8819 - Acc:38.08%: 100%|█| 130/130 [00:41<00:00,  3.1\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.09it/s]\n",
      "Test Epoch 18/20 R@1:59.47% R@10:82.80% R@100:94.72% \n",
      "Train Epoch 19/20 - Loss:3.8974 - Acc:37.86%: 100%|█| 130/130 [00:41<00:00,  3.1\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  8.81it/s]\n",
      "Test Epoch 19/20 R@1:59.49% R@10:82.75% R@100:94.67% \n",
      "Train Epoch 20/20 - Loss:3.9097 - Acc:38.27%: 100%|█| 130/130 [00:41<00:00,  3.1\n",
      "processing test data: 100%|███████████████████| 130/130 [00:14<00:00,  9.07it/s]\n",
      "Test Epoch 20/20 R@1:59.49% R@10:82.82% R@100:94.64% \n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "!python train.py \\\n",
    "--data_path data \\\n",
    "--data_name custom \\\n",
    "--feature_dim 1536 \\\n",
    "--recalls '1,10,100' \\\n",
    "--batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference 1\n",
    "# 학습된 모델(model.pth)을 이용하여 다른 데이터의 feature map 파일(data_base.pth)을 생성\n",
    "\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import Model, set_bn_eval\n",
    "from utils import recall, LabelSmoothingCrossEntropyLoss, BatchHardTripletLoss, ImageReader, MPerClassSampler\n",
    "\n",
    "data_path = 'data'\n",
    "data_name = 'custom'\n",
    "crop_type = 'uncropped'\n",
    "backbone_type = 'resnet50'\n",
    "gd_config = 'SG'\n",
    "feature_dim = 1536\n",
    "smoothing = 0.1\n",
    "temperature = 0.5\n",
    "margin = 0.1\n",
    "recalls = '1,2,4,8'\n",
    "batch_size = 128\n",
    "\n",
    "save_name_pre = '{}_{}_{}_{}_{}_{}_{}_{}_{}'.format(data_name, crop_type, backbone_type, gd_config, feature_dim,\n",
    "                                                        smoothing, temperature, margin, batch_size)\n",
    "\n",
    "test_data_set = ImageReader(data_path, data_name, 'test', crop_type)\n",
    "test_data_loader = DataLoader(test_data_set, batch_size, shuffle=False, num_workers=8)\n",
    "eval_dict = {'test': {'data_loader': test_data_loader}}\n",
    "\n",
    "# model = Model(backbone_type, gd_config, feature_dim, num_classes=len(test_data_set.class_to_idx)).cuda()\n",
    "model = Model(backbone_type, gd_config, feature_dim, num_classes=11318).cuda()\n",
    "\n",
    "pretrained_model_path = 'results/sop_uncropped_resnet50_SG_1536_0.1_0.5_0.1_128_model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # obtain feature vectors for all data\n",
    "    for key in eval_dict.keys():\n",
    "        eval_dict[key]['features'] = []\n",
    "        for inputs, labels in tqdm(eval_dict[key]['data_loader'], desc='processing {} data'.format(key)):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            features, classes = model(inputs)\n",
    "            eval_dict[key]['features'].append(features)\n",
    "        eval_dict[key]['features'] = torch.cat(eval_dict[key]['features'], dim=0)\n",
    "\n",
    "data_base = {}\n",
    "data_base['test_images'] = test_data_set.images\n",
    "data_base['test_labels'] = test_data_set.labels\n",
    "data_base['test_features'] = eval_dict['test']['features']\n",
    "# torch.save(model.state_dict(), 'results/{}_model.pth'.format(save_name_pre))\n",
    "torch.save(data_base, 'results/{}_data_base.pth'.format(save_name_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference 2\n",
    "\n",
    "!python test.py \\\n",
    "--query_img_name 'data/custom/uncropped/251952414262_0.JPG' \\\n",
    "--data_base 'results/custom_uncropped_resnet50_SG_1536_0.1_0.5_0.1_32_data_base.pth' \\\n",
    "--retrieval_num 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7477bf70cb5c199d9228b8f8826ede3070e8bc378a87008e1002dbc64f6a554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
